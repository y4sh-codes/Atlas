{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ac547",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install -q langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to your PDF folder\n",
    "pdf_folder = \"./data\"\n",
    "\n",
    "# Collect all documents from all PDFs\n",
    "all_documents = []\n",
    "\n",
    "for file in os.listdir(pdf_folder):\n",
    "    if file.endswith(\".pdf\"):  # Only process PDFs\n",
    "        loader = PyPDFLoader(os.path.join(pdf_folder, file))\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "# Split the combined documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} documents.\")\n",
    "print(f\"Split into {len(texts)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the new package if not already installed\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Model you want to use\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {'device': 'cuda'}  \n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize embeddings with the new class\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd06eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "# Load variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch values\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "print(f\"Number of text chunks to upload: {len(texts)}\")\n",
    "print(f\"Using embedding model: {embeddings.model_name}\")\n",
    "print(f\"Embedding device: {embeddings.model_kwargs['device']}\")\n",
    "\n",
    "# For large datasets, use smaller batches with progress tracking\n",
    "batch_size = 50  # Can use larger batches with local HuggingFace model\n",
    "total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Will process in {total_batches} batches of up to {batch_size} documents each\")\n",
    "print(\"Processing with local HuggingFace BGE model...\")\n",
    "\n",
    "try:\n",
    "    # Create initial vector store with first batch\n",
    "    first_batch = texts[:batch_size]\n",
    "    print(f\"Creating collection with batch 1/{total_batches} ({len(first_batch)} docs)\")\n",
    "    \n",
    "    qdrant = Qdrant.from_documents(\n",
    "        first_batch,\n",
    "        embeddings,\n",
    "        url=url,\n",
    "        api_key=api_key,\n",
    "        prefer_grpc=False,\n",
    "        timeout=300,  \n",
    "        collection_name=\"vector_db\",\n",
    "        force_recreate=True\n",
    "    )\n",
    "    \n",
    "    uploaded_count = len(first_batch)\n",
    "    print(f\"âœ… Batch 1 complete - {uploaded_count}/{len(texts)} documents uploaded\")\n",
    "    \n",
    "    # Process remaining batches\n",
    "    for i in range(1, total_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(texts))\n",
    "        batch = texts[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"Processing batch {i+1}/{total_batches} ({len(batch)} docs)\")\n",
    "        \n",
    "        # Extract text content and metadata\n",
    "        batch_texts = [doc.page_content for doc in batch]\n",
    "        batch_metadatas = [doc.metadata for doc in batch]\n",
    "        \n",
    "        # Add with retry logic\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                qdrant.add_texts(batch_texts, metadatas=batch_metadatas)\n",
    "                uploaded_count += len(batch)\n",
    "                print(f\"âœ… Batch {i+1} complete - {uploaded_count}/{len(texts)} documents uploaded\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"âš ï¸ Batch {i+1} failed (attempt {attempt+1}), retrying in 2 seconds...\")\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    print(f\"âŒ Batch {i+1} failed after {max_retries} attempts: {e}\")\n",
    "                    raise e\n",
    "        \n",
    "        # No delay needed for local embeddings\n",
    "        \n",
    "    print(f\"\\nðŸŽ‰ Successfully uploaded all {uploaded_count} documents to Qdrant!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error during upload: {e}\")\n",
    "    if \"timeout\" in str(e).lower():\n",
    "        print(\"This is a timeout error. Consider:\")\n",
    "        print(\"1. Using smaller batch sizes\")\n",
    "        print(\"2. Checking your internet connection\")\n",
    "        print(\"3. Upgrading your Qdrant instance if using cloud\")\n",
    "    elif \"cuda\" in str(e).lower() or \"gpu\" in str(e).lower():\n",
    "        print(\"This might be a GPU/CUDA error. Try changing device to 'cpu' in the embeddings model.\")\n",
    "    else:\n",
    "        print(\"Check your Qdrant connection and credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify documents were uploaded to Qdrant\n",
    "print(f\"Using embedding model: {embeddings.model_name}\")\n",
    "print(f\"Model device: {embeddings.model_kwargs['device']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Check collection info\n",
    "    collection_info = qdrant.client.get_collection(\"vector_db\")\n",
    "    print(f\"Collection Name: vector_db\")\n",
    "    print(f\"Points Count: {collection_info.points_count}\")\n",
    "    print(f\"Status: {collection_info.status}\")\n",
    "    \n",
    "    # Check vector configuration\n",
    "    if hasattr(collection_info.config.params, 'vectors'):\n",
    "        if hasattr(collection_info.config.params.vectors, 'size'):\n",
    "            print(f\"Vector Size: {collection_info.config.params.vectors.size}\")\n",
    "        else:\n",
    "            # Handle case where vectors is a dict\n",
    "            vector_config = collection_info.config.params.vectors\n",
    "            if isinstance(vector_config, dict):\n",
    "                for key, config in vector_config.items():\n",
    "                    print(f\"Vector '{key}' Size: {config.size}\")\n",
    "            else:\n",
    "                print(f\"Vector Config: {vector_config}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Test similarity search with ocean-related queries\n",
    "    print(\"Testing similarity search with BGE embeddings...\")\n",
    "    test_queries = [\n",
    "        \"ocean temperature measurements\",\n",
    "        \"marine ecosystem monitoring\", \n",
    "        \"climate change impacts\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        results = qdrant.similarity_search(query, k=2)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  Result {i}:\")\n",
    "            print(f\"    Content: {result.page_content[:150]}...\")\n",
    "            print(f\"    Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully verified {collection_info.points_count} documents in Qdrant!\")\n",
    "    print(f\"âœ… BGE embedding model ({embeddings.model_name}) is working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error verifying documents: {e}\")\n",
    "    print(\"The upload may have failed or the collection doesn't exist.\")\n",
    "    \n",
    "    # Try alternative verification method\n",
    "    try:\n",
    "        print(\"\\nTrying alternative verification...\")\n",
    "        # Just check if we can perform a search\n",
    "        test_results = qdrant.similarity_search(\"test\", k=1)\n",
    "        if test_results:\n",
    "            print(f\"âœ… Collection exists and search works! Found {len(test_results)} result(s)\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Collection exists but no documents found\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Alternative verification also failed: {e2}\")\n",
    "        if \"cuda\" in str(e2).lower():\n",
    "            print(\"ðŸ’¡ Try changing the embedding device from 'cuda' to 'cpu'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
