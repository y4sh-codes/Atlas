{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ac547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üß† 1. Install dependencies\n",
    "# =============================\n",
    "%pip install -r requirements.txt\n",
    "%pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ‚öôÔ∏è 2. Import libraries\n",
    "# =============================\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üîê 3. Set credentials manually (since Kaggle doesn't keep .env files), if trained locally, change code accordingly, \n",
    "# use .env file and dotenv()\n",
    "# =============================\n",
    "# üëâ Replace with your actual credentials (if training on Kaggle since it does not store .env files)\n",
    "os.environ[\"QDRANT_URL\"] = \"https://your-instance.qdrant.tech\"\n",
    "os.environ[\"QDRANT_API_KEY\"] = \"your_qdrant_api_key\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "assert QDRANT_URL and QDRANT_API_KEY and GROQ_API_KEY, \"‚ùå Missing credentials!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üìÇ 4. Load your PDFs\n",
    "# =============================\n",
    "pdf_folder = \"/kaggle/input/pdf-argo\"  # <-- change to your dataset path\n",
    "all_docs = []\n",
    "\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(pdf_folder, file_name))\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source\"] = file_name\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(all_docs)} pages from {pdf_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd06eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ‚úÇÔ∏è 5. Split into text chunks\n",
    "# =============================\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(all_docs)\n",
    "print(f\"‚úÖ Created {len(chunks)} text chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ‚ö° 6. Setup GPU Embedding Model (BGE-M3)\n",
    "# =============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedder = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
    "print(\"‚úÖ Embedding dimension:\", embedder.get_sentence_embedding_dimension())\n",
    "\n",
    "# LangChain wrapper\n",
    "class LangChainBGE(Embeddings):\n",
    "    def __init__(self, model): self.model = model\n",
    "    def __call__(self, texts): return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "    def embed_documents(self, texts): return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "    def embed_query(self, text): return self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "embedding_model = LangChainBGE(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üíæ 7. Upload to Qdrant Cloud\n",
    "# =============================\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    timeout=43200.0  # increase to avoid timeouts\n",
    ")\n",
    "\n",
    "collection_name = \"argo_papers_real\"\n",
    "\n",
    "print(f\"üì¶ Uploading {len(chunks)} chunks to Qdrant Cloud collection '{collection_name}'...\")\n",
    "batch_size = 500\n",
    "for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    Qdrant.from_documents(\n",
    "        documents=batch,\n",
    "        embedding=embedding_model,\n",
    "        url=QDRANT_URL,          \n",
    "        api_key=QDRANT_API_KEY,  \n",
    "        collection_name=collection_name\n",
    "    )\n",
    "print(\"‚úÖ All chunks uploaded successfully to Qdrant Cloud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üß† 8. Build Retriever + LLM (LCEL)\n",
    "# =============================\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    model=\"llama-3.3-70b-versatile\",  # current supported model\n",
    "    temperature=0.1,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "retriever = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embedding_model\n",
    ").as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a scientific assistant. Use the retrieved context to answer the question precisely and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "                                          \n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üí¨ 9. Ask a question\n",
    "# =============================\n",
    "question = \"How have Argo floats improved deep ocean observation?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(\"Q:\", question)\n",
    "print(\"A:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
